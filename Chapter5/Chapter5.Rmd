---
title: "Chapter 5"
author: "Ben Ridenhour"
output:
  html_document:
    df_print: paged
    toc: yes
  html_notebook:
    toc: yes
  pdf_document:
    toc: yes
---


## Choice of Pseudo-RNG

There is not a whole lot to say or know about R's choice of RNG algorithms. The most useful resource for this is the `Random` [help file](https://stat.ethz.ch/R-manual/R-devel/library/base/html/Random.html). Of relevance to our previous discussion is that R uses the Mersenne twister by default. We can see that by: 

```{r}
RNGkind()
```

We can use the same command to switch to other algorithms, of which there are six and an additional user supplied option. For example,

```{r}
RNGkind("Super-Duper") #switch to the (not so) Super-Duper algorithm
RNGkind()
```

We can see the state of the PRNG by doing:

```{r}
.Random.seed
```

And as we've seen previously, we can set the PRNG to a specific state by doing:

```{r}
set.seed(10)
```

Calling a random advances the state of the PRNG by one step:

```{r}
.Random.seed #look at the current state which we just set
runif(1) #throw a random
.Random.seed #look at the state again
set.seed(10) #set the state back to the start of this cycle
.Random.seed #verify that we have the same state
RNGkind("Mers") #use pattern matching to get us a twister again!
RNGkind()
```

## Using the `ecdf()` Function

To use R to generate random number from unusual distributions is fairly easy. We will rely on the ["inverse transform sampling"](https://en.wikipedia.org/wiki/Inverse_transform_sampling) method. The basic idea of this method is that given some cumulative distribution function (CDF), $\Phi(x)$, which describes the quantile $q$ of $x$ in the probability distribution $\phi(x)$. More formally,

$$q = \Phi(x) = \int_-^x\phi(s)ds$$
where $(-)$ indicates the lower bound of distribution. Therfore, by randomly picking a quantile $q$ (which must fall betweeen 0 and 1) from a uniform distribution, we simply apply the inverse of the CDF to $q$ yielding $\Phi^{-1}(q) = x$.

If we data values, we can pretty simply R's built in `ecdf()` function. Let's try doing this for a mixture distribution where we would not typically have a built-in random number generator:   

```{r ecdf}

###  Creating a Mixture Distribution

vals <- c(rnorm(100,0,2), rnorm(100,10,3), rnorm(100,20,5))
hist(vals,30, freq = F,col="red")

CDF <- ecdf(vals)

rands <- unname(quantile(CDF, runif(2000)))

hist(rands,30,col=rgb(0,0,1,0.2), add=T, freq = F)

legend("topright", c("Mixture Sample", "eCDF Sample"), fill = c("red",rgb(0,0,1,0.2)))
```

The basic principle behind what R is doing is to use `rank()` to rank data by their values. Once ranking is done then $\Phi(x) \approx rank(x)/N$. Using values of $\Phi(x)$, R creates a spline that is strictly increasing and passes through all of the data points (see `splinefun()`). The polynomials that create the spline can easily be inverted and used for sampling. 


## Using an Improper Distribution Function

Let's say our distribution is given by:

$$\phi(x) = \sin(x) + 3, \quad x\in[0,2\pi].$$
For a probability distribution to be "proper," the integral over the support domain must equal 1. Clearly, 

$$\int_0^{2\pi}(\sin(x) + 3)dx = 6 \pi;$$
this gives us the normalization constant that we need. Thus, our proper distribution is 

$$ \phi(x) =\frac{1}{6 \pi}\left( \sin(x) + 3\right), \quad x\in[0,2\pi]$$
Now we can simply find the CDF:

$$\Phi(x) = \frac{1}{6\pi}\int_0^x (\sin(s) + 3)ds = \frac{1}{6\pi}(-\cos(x) + 1 + 3x)$$
Let's plot the CDF just to see what it looks like:

```{r CDF}
xVals <- seq(0,2*pi,length=200)
plot(xVals, 1/(6*pi)*(-cos(xVals) +1 + 3*xVals), type = "l", main = "CDF", xlab = "x", ylab = "quantile")
```

Finally, we can now try to solve for the inverse by solving our equation for $x$:

$$q = \frac{1}{6\pi}(-\cos(x) + 1 + 3x) \\
 6\pi q - 1= cos(x) + 3 x,  $$

which cannot easily be inverted (it is an infinite sum). Fear not!!! We can use numerical techniques to find the inverse, specifically, we know that $y = f(x) \implies f(x) - y = 0.$ In other words, given some value $y$, we can numerically find $x$ by finding $f(x) : f(x) - y = 0$. We could use `optim()` to do this, but that is overkill for what we want. Instead, we will use `uniroot()`. To numerically find our inverse for the above function we use:

```{r uniroot}

inverse <- function(y) uniroot(function(x) 1/(6*pi)*(-cos(x) +1 + 3*x) - y, lower = 0, upper = 2*pi)$root 

inverse(0.75)
qVals <-seq(0,1,by=0.01)

plot(qVals,sapply(qVals, inverse), type = "l", main = "Numerical Inverse CDF", ylab = "x", xlab = "quantile")
```

That's some serious mathemagic!!! Now that we have the inverse CDF, we can generate our random numbers using `runif()` just like we did earlier. Let's see what resulting distribution of randoms looks like:

```{r PDF RNG}
rands <- sapply(runif(100000),inverse)
hist(rands, 50)
```



## Sampling From Finite Sets with `sample()` in R 

The are many cases where we might want to sample from a know set of outcomes. Let's try a basic multinomial example. Suppose Bob, Tom, and Jane are all running for class president. Bob is five times as likely to win compared to Tom, and Jane eight times as likely. To create 100 different realization of the election we can do:

```{r}
## Multinomial
elections <- 100
winner <- sample(c("Bob","Tom","Jane"), elections, replace = T, prob = c(5,1,8))
table(winner)/elections
```

Obviously, this is just giving the mean over some number of elections. However, perhaps we want to know the standard error of the means. We could then do:

```{r}
elections <- 100
NSamples <- 30
winner <- sample(c("Bob","Tom","Jane"), elections*NSamples, replace = T, prob = c(5,1,8))
Winner <- matrix(winner, ncol = NSamples)
rowMeans(apply(Winner,2,table)/100) #Average winning percentage
diag(var(t(apply(Winner,2,table)/100)))^0.5 #SD in winning percentage
```

`sample()` is also particularly handy when you want to draw subsets of data in order to perform bootstrapping/resampling. "Leave-n-out" cross-validation is a good example of where we might want to do this. Suppose we have some data, but we suspect there might be some outliers that are influencing the estimation of the mean and variance of the actual distribution. We will use `sample()` to help us out:

```{r}
set.seed(95839)
x <- rnorm(50)
x <- c(x ,rnorm(3,5,1)) #add in some outliers
mean(x) #not near 0!
var(x)^0.5 #not near 1!

### Let's say we leave ~75% out, see how many possibilities that gives
choose(length(x), length(x)%/%4) #more than enough!!!

## Let's try it
NResamp <- 1000
resample <- replicate(NResamp, sample(x,length(x)%/%4)) #integer division is done with "%/%" in R

hist(colMeans(resample), main = "Resampled Means", xlab = "mean")
quantile(colMeans(resample), c(0.05,0.5,0.95)) 

hist(diag(var(resample))^0.5, main = "Resample Variances", xlab = "variance")
quantile(diag(var(resample))^0.5, c(0.05,0.5,0.95))
```

Our cross-validation shows that the variance and the mean are probably lower than the "naive" calculation, and they show that confidence intervals encompass the true mean and variance of the data (0,1 respectively). Let's try sampling with replacement and see what we get there:

```{r}
resample <- replicate(NResamp, sample(x, replace = T))

hist(colMeans(resample), main = "Resampled Means", xlab = "mean")
quantile(colMeans(resample), c(0.05,0.5,0.95)) 

hist(diag(var(resample))^0.5, main = "Resample Variances", xlab = "variance")
quantile(diag(var(resample))^0.5, c(0.05,0.5,0.95))
```

Sampling with replacement improved the estimate of the variance a bit, but it does not help with outliers as much a leave-n-out cross-validation!


## Simulating Equations

One of the tasks we might be faced with is simulating some output variable from various input variables. If we know the distributions of the input variables, we can simulate the distribution of the output variable. Sometimes, we can analytically derive the distribution of the output, but that is not often the case. More commonly, we must use similation to get the desired distribution.  For example, a common solution to an ODE might be:

$$ x(t) = e^{-at}\sin(bt)$$
For simplicity, let's assume that t we are interested $t=1$. Also, assume that we know that $b$ is exponentially distributed with mean 3 and $a$ is normally distributed with a mean of 2 and standard deviation of 0.5. We can readily simulate the distribution of $x$ by doing the following:

```{r}
x <- exp(-1*rexp(10000, 1/3))*sin(rnorm(10000,2,0.5))
hist(x,100)
```

This clearly a distribution for which you will not get an analytical solution, and we can now query the distribution about relevant questions. For example, what is median of $x(t=1)$? (`r median(x)`) Perhaps $x<0$ causes a catastrophic failure; what is the probability that $x < 0$? (`r sum(x < 0)/length(x)`)



